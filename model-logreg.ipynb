{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An alternate baseline model: A horde of logistic regressions\n",
    "\n",
    "We want to predict the Middle Chinese tone, initial, nucleus, and coda. To do this, we first one-hot encode all the features and labels. For clarity, let's define set of labels (`tone_label_departing`, `Karlgren_coda_Å‹`, etc.). as $L$. Now let $m$ be the number of examples we have, $n_X$ be the number of features, and $n_y$ the number of possible labels. We want to predict each of these independently, based on our feature matrix $X$, so we will train $n_y$ logistic regression classifers, where each classifier is trying to predict a different label.\n",
    "\n",
    "- **11/29 fix**: use updated (fixed) data matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import average_precision_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_matrix = pd.read_csv('model/1129-fixed-data-matrix-karlgren.csv').set_index('character')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_columns = [x for x in list(full_matrix.columns) if 'Karlgren' in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have around 15K examples, and we will use a 70/30 train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation metrics\n",
    "def get_f_beta(tn, fp, fn, tp, beta=1):\n",
    "    return (1 + beta ** 2) * tp / ((1 + beta ** 2) * tp + beta ** 2 * fn + fp)\n",
    "def get_precision(tn, fp, fn, tp):\n",
    "    return tp / (tp + fp)\n",
    "def get_recall(tn, fp, fn, tp):\n",
    "    return tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "raw_accs = []\n",
    "eval_func = get_recall\n",
    "for target_column_name in label_columns:\n",
    "    target_column = full_matrix[target_column_name]\n",
    "    if not target_column.sum():\n",
    "        continue\n",
    "        \n",
    "    features = full_matrix.drop(label_columns, axis=1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features,\n",
    "        target_column,\n",
    "        test_size=.3,\n",
    "        random_state=42)\n",
    "\n",
    "    clf = LogisticRegression(solver='liblinear').fit(X_train, y_train)\n",
    "    \n",
    "    raw_train_acc = clf.score(X_train, y_train)\n",
    "    pred = clf.predict(X_test)\n",
    "\n",
    "    cm = confusion_matrix(y_test, pred).flatten()\n",
    "    eval_score = eval_func(*list(cm)) if len(cm) != 1 else 0\n",
    "#     eval_score = 0 if eval_score != eval_score else eval_score # handle NaNs\n",
    "#     print('{:.3f} <- accuracy for {}'.format(eval_score, target_column_name))\n",
    "    metrics.append(eval_score)\n",
    "    raw_accs.append(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw accuracy: 0.988\n",
      "Mean recall: 0.470\n",
      "Median recall: 0.516\n"
     ]
    }
   ],
   "source": [
    "print('Raw accuracy: {:.3f}'.format(np.mean(raw_accs)))\n",
    "print('Mean recall: {:.3f}'.format(np.mean(metrics)))\n",
    "print('Median recall: {:.3f}'.format(np.median(metrics)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is our accuracy so high, yet our recall is so low?\n",
    "\n",
    "Our strategy was to instantiate a logistic regression classifier for every one-hot encoded label category. But since there were $>100$ label categories, the column vector corresponding to each label category was very sparse. Most of our classifiers had no reason to truly learn; all they needed to do was to predict $0$. For every example, we need a way to force our model to predict exactly 1 onset, exactly 1 nucleus, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
